{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f85b068",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0430c50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('oil_price_cleaned.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "price = data['Price']\n",
    "\n",
    "# Split into train and test sets\n",
    "train_size = int(len(price) * 0.8)\n",
    "train, test = price[0:train_size], price[train_size:len(price)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9fed9d",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9af7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First-order differencing\n",
    "diff1_train = train.diff().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad4e0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADF test\n",
    "adf_result = adfuller(diff1_train, autolag='AIC')\n",
    "adf_stat = adf_result[0]\n",
    "adf_pvalue = adf_result[1]\n",
    "adf_crit = adf_result[4]\n",
    "\n",
    "# Print results\n",
    "print(\"ADF Statistic:\", adf_stat)\n",
    "print(\"p-value:\", adf_pvalue)\n",
    "print(\"Critical Values:\", adf_crit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b84b7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ACF and PACF to determine the values of p and q in the ARIMA(p,d,q) model.\n",
    "# ACF and PACF plots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14,4))\n",
    "\n",
    "plot_acf(diff1_train, lags=40, ax=axes[0])\n",
    "axes[0].set_title(\"ACF (Diff(1))\")\n",
    "\n",
    "plot_pacf(diff1_train, lags=40, ax=axes[1], method='ywm')\n",
    "axes[1].set_title(\"PACF (Diff(1))\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905ead75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteration to select the best model based on AIC/BIC\n",
    "p_max = 5   # Maximum order of p, can be adjusted\n",
    "q_max = 5   # Maximum order of q, can be adjusted\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "results = []\n",
    "for p in range(p_max+1):\n",
    "    for q in range(q_max+1):\n",
    "        try:\n",
    "            model = ARIMA(train, order=(p,1,q))   # d=1\n",
    "            fitted = model.fit()\n",
    "            results.append([p, q, fitted.aic, fitted.bic])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "# Organize results into a table\n",
    "results_df = pd.DataFrame(results, columns=[\"p\", \"q\", \"AIC\", \"BIC\"])\n",
    "results_df = results_df.sort_values(by=\"AIC\") # chose AIC\n",
    "\n",
    "print(\"Sorted by AIC:\")\n",
    "print(results_df.head(10))\n",
    "\n",
    "print(\"Best model (minimum AIC):\", results_df.iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670a77c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "# Fit the best model (using the best p,q from previous step)\n",
    "best_p, best_q = results_df.iloc[0][[\"p\",\"q\"]]\n",
    "best_model = ARIMA(train, order=(int(best_p),1,int(best_q)))\n",
    "best_fit = best_model.fit()\n",
    "\n",
    "# Get residuals\n",
    "residuals = best_fit.resid\n",
    "\n",
    "# QQ Plot (normality test)\n",
    "plt.figure(figsize=(6,6))\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title(\"QQ Plot of Residuals\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Autocorrelation function of residuals (ACF)\n",
    "plt.figure(figsize=(6,4))\n",
    "plot_acf(residuals, lags=40)\n",
    "plt.title(\"ACF of Residuals\")\n",
    "plt.show()\n",
    "\n",
    "# Ljung-Box test (white noise test)\n",
    "ljung_res = acorr_ljungbox(residuals, lags=[10,20,30], return_df=True)\n",
    "print(\"Ljung-Box test results:\")\n",
    "print(ljung_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278f43cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = (5, 1, 4)\n",
    "print(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc58695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Multi-step prediction: predict the entire test set length at once =========\n",
    "def arima_multi_step(train_series, steps, order):\n",
    "    model = ARIMA(train_series, order=order)\n",
    "    fit = model.fit()\n",
    "    # Can also use get_forecast to get confidence intervals\n",
    "    fc_obj = fit.get_forecast(steps=steps)\n",
    "    mean_fc = fc_obj.predicted_mean\n",
    "    conf_int = fc_obj.conf_int(alpha=0.05)  # 95% interval\n",
    "    mean_fc.index = pd.Index(test.index)    # Align with test set index (to be safe)\n",
    "    conf_int.index = pd.Index(test.index)\n",
    "    conf_int.columns = ['lower', 'upper']\n",
    "    return mean_fc.rename(\"multi_step_pred\"), conf_int\n",
    "\n",
    "multi_pred, multi_ci = arima_multi_step(train, steps=len(test), order=order)\n",
    "# Plot\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(test.index, test.values, label=\"Actual\")\n",
    "plt.plot(multi_pred.index, multi_pred.values, label=\"Multi-step Forecast\")\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"Price\"); plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc260f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Single-step prediction (expanding window) =========\n",
    "def arima_single_step_expanding(train_series, test_series, order):\n",
    "    history = train_series.copy()\n",
    "    preds = []\n",
    "\n",
    "    for ts, y_true in zip(test_series.index, test_series.values):\n",
    "        y_hist = history                      # Key for expanding window: don't truncate history\n",
    "        fit = ARIMA(y_hist, order=order).fit()\n",
    "        y_hat = float(fit.forecast(steps=1))\n",
    "        preds.append(y_hat)\n",
    "        # Use \"actual values\" to advance\n",
    "        history = pd.concat([history, pd.Series([y_true], index=[ts])])\n",
    "\n",
    "    return pd.Series(preds, index=test_series.index, name=\"single_step_expanding\")\n",
    "\n",
    "single_pred = arima_single_step_expanding(train, test, order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f0b9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(test.index, test.values, label=\"Actual\")\n",
    "plt.plot(single_pred.index, single_pred.values, label=\"One-step Forecast\")\n",
    "plt.xlabel(\"Date\"); plt.ylabel(\"Price\"); plt.legend(); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6596ae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_mape(y_true, y_pred, eps=1e-8):\n",
    "    y_true = np.asarray(y_true, dtype=float)\n",
    "    y_pred = np.asarray(y_pred, dtype=float)\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "    denom = np.where(np.abs(y_true) < eps, eps, np.abs(y_true))\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / denom)) * 100\n",
    "    return rmse, mape\n",
    "\n",
    "rmse_single, mape_single = rmse_mape(test.values, single_pred.values)\n",
    "rmse_multi,  mape_multi  = rmse_mape(test.values, multi_pred.values)\n",
    "print(f\"[Single-step (Expanding)] RMSE = {rmse_single:.4f}, MAPE = {mape_single:.2f}%\")\n",
    "print(f\"[Multi-step] RMSE = {rmse_multi:.4f}, MAPE = {mape_multi:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a8da77",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8cbc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "\n",
    "# ========== Direction Label Construction ==========\n",
    "last_train = float(train.iloc[-1])\n",
    "\n",
    "# True direction: p_t - p_{t-1} (using last_train as the first previous value)\n",
    "y_true_dir = []\n",
    "prev_true = last_train\n",
    "for ts, y in test.items():\n",
    "    y_true_dir.append(1 if (y - prev_true) > 0 else 0)\n",
    "    prev_true = y\n",
    "y_true_dir = np.array(y_true_dir, dtype=int)\n",
    "\n",
    "# Single-step prediction direction: hat_p_t - p_{t-1} (advancing with actual values)\n",
    "y_pred_single_dir = []\n",
    "prev_true = last_train\n",
    "for ts, y_hat in single_pred.items():\n",
    "    y_pred_single_dir.append(1 if (y_hat - prev_true) > 0 else 0)\n",
    "    prev_true = float(test.loc[ts])  # advance with actual values\n",
    "y_pred_single_dir = np.array(y_pred_single_dir, dtype=int)\n",
    "\n",
    "# Multi-step prediction direction: hat_p_t - hat_p_{t-1} (advancing with predicted values)\n",
    "y_pred_multi_dir = []\n",
    "prev_pred = last_train\n",
    "for ts, y_hat in multi_pred.items():\n",
    "    y_pred_multi_dir.append(1 if (y_hat - prev_pred) > 0 else 0)\n",
    "    prev_pred = float(y_hat)         # advance with predicted values\n",
    "y_pred_multi_dir = np.array(y_pred_multi_dir, dtype=int)\n",
    "\n",
    "# ========== Print Evaluation (Accuracy and F1 Score) ==========\n",
    "def print_report(y_true, y_pred, title):\n",
    "    print(\"=\"*60)\n",
    "    print(title)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print_report(y_true_dir, y_pred_multi_dir,  \"Multi-step (Direction) Evaluation\")\n",
    "print_report(y_true_dir, y_pred_single_dir, \"Single-step (Direction) Evaluation\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hxt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
